{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick review on Quasi Newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical applications one often wants to find the optimum of a given function, $f(x)$. Often, $x$ belongs to a high dimensional space. \n",
    "\n",
    "A grid search might be too expensive, and ideally the function is *well behaved* such that we want to use the most out of our function. \n",
    "\n",
    "The problem we want to solve is to find $x^*$ such that it minimises the $f(\\cdot)$, *ie* \n",
    "\n",
    "$$ x^* = \\arg \\min_{x \\in \\mathbb{R}^n}  \\, f(x). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods\n",
    "\n",
    "Use an iterative numerical algorithm where we treat each iterand as our *best guess* at step $n$. Let $x_k$ be such iteration which we would hope to converge to the best point $x^*$.\n",
    "\n",
    "The iterations are done in such a way that they satisfy \n",
    "\n",
    "$$ f(x_{k+1}) < f(x_k). $$ \n",
    "\n",
    "**Netwon's method** assumes a well behaved function. This means that it assumes our function to be *twice-differentiable*. That means in turn, that a **quadratic local approximation** is good enough at any given point of the domain. \n",
    "\n",
    "Centered at $x$, we use the second order Taylor expansion \n",
    "\n",
    "$$ f(x + p) \\approx f(x) + p^\\top \\nabla f(x) + \\frac{1}{2} p^\\top \\nabla^2 f(x) \\,  p, $$\n",
    "\n",
    "where $\\nabla f(x)$, $\\nabla^2 f(x)$ and $p$ denote the gradient, the Hessian and the direction of change with respect to the point of origin $x$, in which both gradient and Hessian are evaluated. \n",
    "\n",
    "In our iterative approach, this is can be rewritten as \n",
    "\n",
    "$$ f(x_k + p) \\approx f(x_k) + p^\\top \\nabla f(x_k) + \\frac{1}{2} p^\\top \\nabla^2 f(x_k) \\,  p, $$\n",
    "\n",
    "where it is easily seen that the each new point is defined as \n",
    "\n",
    "$$ x_{k+1} = x_k + p. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose the direction?\n",
    "\n",
    "We assume that the quadratic model is good enough. And we find $p_k$ such that it minimises the quadratic approximation model \n",
    "\n",
    "$$ \n",
    "m_k(p) =  f(x_k) + p^\\top \\nabla f(x_k) + \\frac{1}{2} p^\\top \\nabla^2 f(x_k) \\,  p.\n",
    "$$\n",
    "\n",
    "First order conditions imply that \n",
    "\n",
    "$$ \\nabla m_k(p_k) = 0, $$ \n",
    "\n",
    "and by second order conditions we expect that $\\nabla^2 f(x_k)$ is a symmetric positive definite matrix.\n",
    "\n",
    "This implies that \n",
    "\n",
    "$$ p_k = - H_k^{-1} g_k, $$\n",
    "\n",
    "where $H_k$ and $g_k$ denote the **Hessian and gradient of $f$ evaluated at  $x_k$**, respectively.\n",
    "\n",
    "This approach might work well, but to further guarantee good convergence conditions we use a fraction $\\alpha$ of such descent direction. \n",
    "\n",
    "$$ x_{k+1} = x_k - \\alpha \\, H_k^{-1} g_k, $$ \n",
    "\n",
    "where is $\\alpha$ is commonly known as the **learning rate** in Machine learning or the **step length** in the Optimisation community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Newton's method\n",
    "\n",
    "To compute the gradient might be troublesome, but usually one is willing to spend the computational effort. \n",
    "\n",
    "For the Hessian in the other hand, it probably could be computationally too burdersome, or the size of such matrix might be too expensive to allocate, thus calculating it's inverse might be too expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives\n",
    "\n",
    "An alternative is to use an approximation to the Hessian, often denoted as $B$, such that it posses some ideal properties of the true Hessian.\n",
    "\n",
    "First, we expect the approximated Hessian to satisfy the **secant condition**. That means, to satisfy the equation \n",
    "\n",
    "$$ \\nabla f(x_k+p) = \\nabla f(x_k) + B \\, p, $$ \n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$ s_k = B^{â€“1} y_k,$$\n",
    "\n",
    "where $s_k$ denotes the difference between succesive iterations (or the step direction) and $y_k$ the difference in gradients. \n",
    "\n",
    "We further ask our estimation of the Hessian to preserve symmetry and be an update from the previous one by the smallest change possible. \n",
    "\n",
    "This means that, we look for $B_{k+1}$ such that it solves the following optimisation problem\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\begin{array}{ll}\n",
    "\\min_{B^{-1}} & || B^{-1} - B_k^{-1} ||^2 \\\\\n",
    "\\text{s.t.} & s_k = B^{-1} \\, y_k \\\\\n",
    "& B^{-1} \\text{is symmetric}.\n",
    "\\end{array}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The solution to this problem is \n",
    "\n",
    "$$ B_{k+1} = B_k + \\frac{y_k\\,y_k^\\top}{y_k^\\top s_k} - \\frac{B_k s_k \\, (B_k s_k)^\\top}{s_k^\\top B_k s_k}. $$\n",
    "\n",
    "However, it is truely useful when used alognside the **Sherman-Morrison formula**, since it can directly be used to compute the inverse with \n",
    "\n",
    "$$ B_{k+1}^{-1} = \\left( I - \\frac{s_ky_k^\\top}{y_k^\\top s_k}\\right) B_{k}^{-1} \\left( I - \\frac{y_ks_k^\\top}{s_k^\\top y_k} \\right) + \\frac{s_ks_k^\\top}{y_k^\\top s_k}, $$\n",
    "\n",
    "with just matrix-vectors operators!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [blog](http://aria42.com/blog/2014/12/understanding-lbfgs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
